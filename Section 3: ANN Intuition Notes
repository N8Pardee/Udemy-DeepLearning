The Neuron:
  - 1st step: neuron (node) takes in input values with weights (synapse) and produces output signal
  - 2nd step: takes weighted sum of all input values. Summation i = 1 to m of weight_i * x_i
  - 3rd step: neuron will or will not process and pass on the output signal.

  x1 \
  x2 -  neuron -- y
  x3 /

  - We need to standardized or normalize our variables, meaning mean of zero or variance 1
  - normalize by subtracting minimum value and dividing maximum - minimum to find the range of values between 0 to 1
  - values will go into NN and influenced by weights which is why it is important to standardize the variables

  addnl reading Efficient BackProp by Yann LeCun (1998)
    Link: http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf

  -oputput variables can be continuous(price), Binary(yes/no),categorical
  -single observation from input (one row from dataset) and on ouput side there is a single observation for that same row. Same Observation

  -Synapses are assigned weights.
  -weights are crucial for ANN. Weights are how the NN learns. Gradient, Descendt, backprop come into play here

Activation Function:
-4 different types of activation functions

Threshold function:
  -x axis is weighted sum of inputs and y axis is values from 0 to 1
  - if the value is less than 0, then threshold fn passes on 0, if >= 0 then it passes a 1. 
  -Either yes or no, 1 or 0, etc

Sigmoid function:
- x axis is sum of weighted inputs
-y axis is 1/(1+e^-x) where x is the value of the weighted sum



  
  
